{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment1_train.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "q30fJVFJW3bv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "## there are two zips uploaded,assignment.zip or assignment(1).zip,please use other if binary compression error occurs!!\n",
        "!unzip '/content/drive/My Drive/Assignment.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5l9YHRp10fd3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import h5py\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import optimizers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Conv2D,MaxPooling2D,Flatten,Dropout,Flatten,Reshape,ZeroPadding2D\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense \n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "##config = tf.ConfigProto()\n",
        "config = tf.ConfigProto(device_count ={'GPU': 0}) \n",
        "config.gpu_options.allow_growth = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsSeRO_R1LYf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights_path = '/content/drive/My Drive/model_weights.h5'\n",
        "\n",
        "# dimensions of our images. the model downloaded was trained on images resized to 150*150 images\n",
        "img_width, img_height = 800,800 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xy9HU5H61LdF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_dir = '/content/Assignment/Input/Dataset/train' ## train data\n",
        "validation_data_dir = '/content/Assignment/Input/Dataset/validation' ## augmented validation data created by augment.py\n",
        "nb_train_samples = 50\n",
        "nb_validation_samples = 30\n",
        "nb_epoch = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3h4ulid1LhE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build the model for dev\n",
        "model = Sequential()\n",
        "## although all images are same size, zero padding doesnt hurt if were dealing with unknown test dimensions. also helps with image warping \n",
        "model.add(ZeroPadding2D((1, 1), input_shape=(img_width, img_height,1)))\n",
        "## first convolution stage\n",
        "model.add(Conv2D(64, 3, 3, activation='relu', name='conv1_1'))\n",
        "model.add(ZeroPadding2D((1, 1)))\n",
        "model.add(Conv2D(64, 3, 3, activation='relu', name='conv1_2'))\n",
        "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "## second convolution stage\n",
        "model.add(ZeroPadding2D((1, 1)))\n",
        "model.add(Conv2D(256, 3, 3, activation='relu', name='conv2_1'))\n",
        "model.add(ZeroPadding2D((1, 1)))\n",
        "model.add(Conv2D(256, 3, 3, activation='relu', name='conv2_2'))\n",
        "model.add(ZeroPadding2D((1, 1)))\n",
        "model.add(Conv2D(256, 3, 3, activation='relu', name='conv2_3'))\n",
        "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "## flatten the output from layer above to feed it to decision making dense layers\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "## softmax because it is a multi-class problem\n",
        "model.add(Dense(4, activation='softmax'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33TxhPNqJOSI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0kkYiA54PVD",
        "colab_type": "text"
      },
      "source": [
        "load the weights of the VGG16 networks(trained on ImageNet, won the ILSVRC competition in 2014) note: when there is a complete match between your model definition and your weight savefile, you can simply call model.load_weights(filename)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTLnCtLj4K8k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## weights were reloaded to faciliate fine tuning\n",
        "model.load_weights(weights_path)\n",
        "print('Model loaded.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4LXPVbZukGc",
        "colab_type": "text"
      },
      "source": [
        "# Matthews Correlation degree\n",
        "Matthews_correlation is often regarded as the best measure for binary classification\n",
        "as latest versions of keras removed this useful metrics, I have written a version following source code of previous versions of keras\n",
        "A value close to 1 is often regarded as best\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06LaRne0Tg4U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mcor(y_true, y_pred):\n",
        "     #matthews_correlation\n",
        "     y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
        "     y_pred_neg = 1 - y_pred_pos\n",
        " \n",
        " \n",
        "     y_pos = K.round(K.clip(y_true, 0, 1))\n",
        "     y_neg = 1 - y_pos\n",
        " \n",
        " \n",
        "     tp = K.sum(y_pos * y_pred_pos)\n",
        "     tn = K.sum(y_neg * y_pred_neg)\n",
        " \n",
        " \n",
        "     fp = K.sum(y_neg * y_pred_pos)\n",
        "     fn = K.sum(y_pos * y_pred_neg)\n",
        " \n",
        " \n",
        "     numerator = (tp * tn - fp * fn)\n",
        "     denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
        " \n",
        " \n",
        "     return numerator / (denominator + K.epsilon())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0EMDIfeuvCH",
        "colab_type": "text"
      },
      "source": [
        "# F-1 Score metric \n",
        "\n",
        "F1 score is often a good measure to gauge a model performance\n",
        "The function below calculates precision and recall and then calculates\n",
        "F1 score using the formula mentioned in the return method\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFPwv2M4ThET",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def f1(y_true, y_pred):\n",
        "    def recall(y_true, y_pred):\n",
        "        \"\"\"Recall metric.\n",
        "\n",
        "        Only computes a batch-wise average of recall.\n",
        "\n",
        "        Computes the recall, a metric for multi-label classification of\n",
        "        how many relevant items are selected.\n",
        "        \"\"\"\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "    def precision(y_true, y_pred):\n",
        "        \"\"\"Precision metric.\n",
        "\n",
        "        Only computes a batch-wise average of precision.\n",
        "\n",
        "        Computes the precision, a metric for multi-label classification of\n",
        "        how many selected items are relevant.\n",
        "        \"\"\"\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "    precision = precision(y_true, y_pred)\n",
        "    recall = recall(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoas4bSJ4hOm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compile the model with a Adamax optimizer\n",
        "\n",
        "# and a very slow learning rate.\n",
        "## adam rate set is 1e-4 was used for training first set of model weights,the model was finetuned with adamax with a decay of \n",
        "## 0.002 not written in the code\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "          optimizer=optimizers.Adamax(lr=1e-4),\n",
        "          metrics=['accuracy',f1,mcor])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5hEkY4F4hRJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prepare data augmentation configuration\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4no8pbm64hTu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_data_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=32,\n",
        "    class_mode= 'categorical',color_mode='grayscale')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    validation_data_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=32,\n",
        "    class_mode= 'categorical',color_mode='grayscale')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Usdmjz8a42-e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tune the model\n",
        "tf.Session(config = config)\n",
        "model.fit_generator(\n",
        "    train_generator,\n",
        "    samples_per_epoch=nb_train_samples,\n",
        "    nb_epoch=nb_epoch,\n",
        "    validation_data=validation_generator,\n",
        "    nb_val_samples=nb_validation_samples)\n",
        "\n",
        "model.save_weights(\"/content/drive/My Drive/model_weights.h5\")\n",
        "model.save(\"/content/drive/My Drive/category_model.h5\", True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NG3my5sr9VR",
        "colab_type": "text"
      },
      "source": [
        "As shown in results above the model gives good accuracy on training(around 95%) and competitive performance on validation set(contains many stray pixels) "
      ]
    }
  ]
}